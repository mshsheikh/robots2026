"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_course=globalThis.webpackChunkphysical_ai_humanoid_robotics_course||[]).push([[101],{4010:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"week8-ai-integration","title":"Week 8: AI Integration in Humanoid Systems","description":"Introduction to AI for Humanoid Robots","source":"@site/docs/week8-ai-integration.md","sourceDirName":".","slug":"/week8-ai-integration","permalink":"/robots2026/docs/week8-ai-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/mshsheikh/ai-humanoid-robotics/tree/master/docs/week8-ai-integration.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9},"sidebar":"tutorialSidebar","previous":{"title":"Week 7: Humanoid Robot Hardware Design","permalink":"/robots2026/docs/week7-humanoid-hardware"},"next":{"title":"Week 9: Human-Robot Interaction","permalink":"/robots2026/docs/week9-human-robot-interaction"}}');var r=i(4848),t=i(8453);const o={sidebar_position:9},l="Week 8: AI Integration in Humanoid Systems",a={},c=[{value:"Introduction to AI for Humanoid Robots",id:"introduction-to-ai-for-humanoid-robots",level:2},{value:"Learning Paths by Experience Level",id:"learning-paths-by-experience-level",level:2},{value:"Newbie in Tech",id:"newbie-in-tech",level:3},{value:"What &quot;AI inside a robot&quot; Actually Means",id:"what-ai-inside-a-robot-actually-means",level:4},{value:"Perception \u2192 Decision \u2192 Action Loop (Intuitive Explanation)",id:"perception--decision--action-loop-intuitive-explanation",level:4},{value:"Examples of AI in Daily-Life Robots",id:"examples-of-ai-in-daily-life-robots",level:4},{value:"Learning Objectives",id:"learning-objectives",level:4},{value:"Guided Thought Experiments",id:"guided-thought-experiments",level:4},{value:"Glossary",id:"glossary",level:4},{value:"Junior / Beginner",id:"junior--beginner",level:3},{value:"Learning Objectives",id:"learning-objectives-1",level:4},{value:"Where ML Models Live in a Robot Stack",id:"where-ml-models-live-in-a-robot-stack",level:4},{value:"Difference Between Rules, Classical Control, and Learning",id:"difference-between-rules-classical-control-and-learning",level:4},{value:"Intro to Vision-Language-Action (VLA)",id:"intro-to-vision-language-action-vla",level:4},{value:"Simple ROS 2 + AI Pipeline Diagram (Described in Text)",id:"simple-ros-2--ai-pipeline-diagram-described-in-text",level:4},{value:"Common Integration Patterns",id:"common-integration-patterns",level:4},{value:"Mid-Level Engineer",id:"mid-level-engineer",level:3},{value:"Learning Objectives",id:"learning-objectives-2",level:4},{value:"Integrating Perception Models with Planners",id:"integrating-perception-models-with-planners",level:4},{value:"Latency, Throughput, and Hardware Constraints",id:"latency-throughput-and-hardware-constraints",level:4},{value:"Model Deployment (Edge vs Cloud)",id:"model-deployment-edge-vs-cloud",level:4},{value:"Failure Modes and Fallbacks",id:"failure-modes-and-fallbacks",level:4},{value:"Code Snippets",id:"code-snippets",level:4},{value:"Model Monitoring and Safety",id:"model-monitoring-and-safety",level:4},{value:"Senior / Executive",id:"senior--executive",level:3},{value:"Learning Objectives",id:"learning-objectives-3",level:4},{value:"AI System Architecture for Humanoids",id:"ai-system-architecture-for-humanoids",level:4},{value:"Safety, Verification, and Monitoring",id:"safety-verification-and-monitoring",level:4},{value:"Cost, Scalability, and Update Strategy",id:"cost-scalability-and-update-strategy",level:4},{value:"Evaluation Metrics (Task Success, Robustness)",id:"evaluation-metrics-task-success-robustness",level:4},{value:"Build vs Buy Considerations",id:"build-vs-buy-considerations",level:4},{value:"Deployment Readiness Checklist",id:"deployment-readiness-checklist",level:4}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"week-8-ai-integration-in-humanoid-systems",children:"Week 8: AI Integration in Humanoid Systems"})}),"\n",(0,r.jsx)(n.h2,{id:"introduction-to-ai-for-humanoid-robots",children:"Introduction to AI for Humanoid Robots"}),"\n",(0,r.jsx)(n.p,{children:"This week covers the essential concepts of integrating artificial intelligence into humanoid robotic systems."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Learning from Demonstration"}),": Techniques for humanoid robots to learn new behaviors from human demonstrations, including imitation learning and behavioral cloning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Reinforcement Learning Applications"}),": Using RL for motor skill acquisition, gait optimization, and adaptive behavior in humanoid robots with continuous action spaces."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Planning and Reasoning"}),": Integrating high-level AI planning with low-level motor control for complex task execution and problem solving."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This knowledge enables the development of more autonomous and adaptable humanoid robots."}),"\n",(0,r.jsx)(n.h2,{id:"learning-paths-by-experience-level",children:"Learning Paths by Experience Level"}),"\n",(0,r.jsx)(n.h3,{id:"newbie-in-tech",children:"Newbie in Tech"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Audience"}),": AI/robotics newcomer"]}),"\n",(0,r.jsx)(n.h4,{id:"what-ai-inside-a-robot-actually-means",children:'What "AI inside a robot" Actually Means'}),"\n",(0,r.jsx)(n.p,{children:"AI in a robot means the robot can process information, make decisions, and learn from experience rather than just following pre-programmed instructions. Think of it like having a brain that can think and learn, rather than just a set of reflexes. The AI helps the robot understand what it sees, hears, or senses, decide what to do next, and then carry out that action."}),"\n",(0,r.jsx)(n.h4,{id:"perception--decision--action-loop-intuitive-explanation",children:"Perception \u2192 Decision \u2192 Action Loop (Intuitive Explanation)"}),"\n",(0,r.jsx)(n.p,{children:"The robot follows a simple cycle:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perceive"}),": Use sensors (cameras, microphones, touch sensors) to gather information about the world"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Decide"}),": Process that information using AI to figure out what to do"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Act"}),": Move motors or speak to interact with the environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Repeat"}),": Continuously sense, think, and act to achieve goals"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"examples-of-ai-in-daily-life-robots",children:"Examples of AI in Daily-Life Robots"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Roomba Vacuum"}),": Uses AI to navigate around obstacles and clean efficiently"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Smart Assistants"})," (Alexa, Siri): Understand speech and respond appropriately"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Self-Driving Cars"}),": Process visual and sensor data to navigate safely"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Warehouse Robots"}),": Use AI to pick and place items efficiently"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand what AI means in the context of robotics"}),"\n",(0,r.jsx)(n.li,{children:"Recognize the basic perception-decision-action cycle in robots"}),"\n",(0,r.jsx)(n.li,{children:"Identify examples of AI-powered robots in daily life"}),"\n",(0,r.jsx)(n.li,{children:"Appreciate how AI makes robots more capable than simple machines"}),"\n",(0,r.jsx)(n.li,{children:"Distinguish between rule-based and AI-powered robot behavior"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"guided-thought-experiments",children:"Guided Thought Experiments"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"The Smart Helper Robot"})," (Time estimate: 15-20 minutes)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Imagine a robot that helps with household tasks"}),"\n",(0,r.jsx)(n.li,{children:"Consider how it would use sensors to perceive a messy room"}),"\n",(0,r.jsx)(n.li,{children:"Think about how it would decide what to clean first"}),"\n",(0,r.jsx)(n.li,{children:"Visualize how it would act to accomplish the cleaning"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Learning vs. Programming"})," (Time estimate: 20-25 minutes)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Compare a traditional programmed robot that follows fixed rules"}),"\n",(0,r.jsx)(n.li,{children:"Contrast with an AI robot that can learn and adapt"}),"\n",(0,r.jsx)(n.li,{children:"Consider how each would handle unexpected situations"}),"\n",(0,r.jsx)(n.li,{children:"Reflect on the advantages of AI-powered flexibility"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"AI Decision-Making Process"})," (Time estimate: 20-25 minutes)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Think about how a robot would decide whether to approach a person"}),"\n",(0,r.jsx)(n.li,{children:"Consider what information it would need to process"}),"\n",(0,r.jsx)(n.li,{children:"Imagine the factors it would weigh in making this decision"}),"\n",(0,r.jsx)(n.li,{children:"Reflect on the complexity of human-like decision making"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"glossary",children:"Glossary"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VLA (Vision-Language-Action)"}),": AI models that can see, understand language, and take physical actions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model"}),": A mathematical representation that AI uses to make predictions or decisions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": The process of using a trained AI model to make predictions on new data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Policy"}),": A strategy or set of rules that determines what actions an AI agent should take"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perception"}),": The ability to interpret sensory information from the environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reinforcement Learning"}),": Learning through trial and error with rewards for good behavior"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deep Learning"}),": A subset of machine learning using neural networks with multiple layers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Training"}),": The process of teaching an AI model using examples or experience"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"junior--beginner",children:"Junior / Beginner"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Audience"}),": Early AI/robotics learner"]}),"\n",(0,r.jsx)(n.h4,{id:"learning-objectives-1",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Identify where machine learning models fit in the robot software stack"}),"\n",(0,r.jsx)(n.li,{children:"Distinguish between rule-based systems, classical control, and learning-based approaches"}),"\n",(0,r.jsx)(n.li,{children:"Understand the concept of Vision-Language-Action (VLA) models"}),"\n",(0,r.jsx)(n.li,{children:"Explain how AI integrates with ROS 2 communication systems"}),"\n",(0,r.jsx)(n.li,{children:"Apply basic concepts of AI-robotics integration"}),"\n",(0,r.jsx)(n.li,{children:"Connect AI capabilities to hardware and sensor systems"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"where-ml-models-live-in-a-robot-stack",children:"Where ML Models Live in a Robot Stack"}),"\n",(0,r.jsx)(n.p,{children:"Machine learning models typically reside in the perception and decision-making layers of the robot software stack:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perception Layer"}),": Vision models for object detection, speech recognition models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Planning Layer"}),": Path planning and task planning models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Control Layer"}),": Models for motor control and motion generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Interaction Layer"}),": Natural language processing and social interaction models"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"difference-between-rules-classical-control-and-learning",children:"Difference Between Rules, Classical Control, and Learning"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rule-Based Systems"}),': Follow explicit "if-then" statements programmed by humans']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Classical Control"}),": Use mathematical models and feedback loops to control robot behavior"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Learning-Based Systems"}),": Learn patterns and behaviors from data, adapting over time"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"intro-to-vision-language-action-vla",children:"Intro to Vision-Language-Action (VLA)"}),"\n",(0,r.jsx)(n.p,{children:"VLA models combine three capabilities:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision"}),": Understanding what the robot sees through cameras"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language"}),": Processing and generating human language for communication"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action"}),": Converting understanding into physical robot movements"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These models enable robots to follow natural language commands while understanding their visual environment."}),"\n",(0,r.jsx)(n.h4,{id:"simple-ros-2--ai-pipeline-diagram-described-in-text",children:"Simple ROS 2 + AI Pipeline Diagram (Described in Text)"}),"\n",(0,r.jsx)(n.p,{children:"The pipeline follows this flow:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Data Collection"}),": Cameras, microphones, and other sensors publish data as ROS 2 messages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"AI Processing Node"}),": Subscribes to sensor messages, runs inference on ML models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Decision Output"}),": Publishes commands or plans as ROS 2 messages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Control Node"}),": Subscribes to AI decisions and executes on hardware"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feedback Loop"}),": Sensor data continuously updates the AI system"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"common-integration-patterns",children:"Common Integration Patterns"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Publisher-Subscriber"}),": AI nodes process sensor data and publish commands"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action Servers"}),": For long-running tasks with feedback and cancellation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Services"}),": For on-demand processing like object recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Parameter Servers"}),": For configuring AI model parameters at runtime"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"mid-level-engineer",children:"Mid-Level Engineer"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Audience"}),": AI/robotics practitioner"]}),"\n",(0,r.jsx)(n.h4,{id:"learning-objectives-2",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Integrate perception models with motion planning and control systems"}),"\n",(0,r.jsx)(n.li,{children:"Optimize AI inference for real-time robotic applications"}),"\n",(0,r.jsx)(n.li,{children:"Design model deployment strategies for edge vs cloud computing"}),"\n",(0,r.jsx)(n.li,{children:"Implement robust failure detection and fallback mechanisms"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate and monitor AI model performance in robotic systems"}),"\n",(0,r.jsx)(n.li,{children:"Handle hardware constraints and resource management"}),"\n",(0,r.jsx)(n.li,{children:"Design safe AI-robotics integration patterns"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"integrating-perception-models-with-planners",children:"Integrating Perception Models with Planners"}),"\n",(0,r.jsx)(n.p,{children:"Perception models provide critical information for planning systems:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object Detection"}),": Identifies objects and their poses for manipulation planning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scene Understanding"}),": Provides semantic information for navigation planning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"State Estimation"}),": Tracks robot and environment state for dynamic planning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Uncertainty Quantification"}),": Provides confidence estimates for safe planning"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"latency-throughput-and-hardware-constraints",children:"Latency, Throughput, and Hardware Constraints"}),"\n",(0,r.jsx)(n.p,{children:"Critical performance factors for AI in robotics:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference Latency"}),": AI models must respond quickly enough for real-time control"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Throughput Requirements"}),": Processing must keep up with sensor data rates"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware Constraints"}),": Power, compute, and memory limitations on robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Requirements"}),": Control loops often require 100Hz+ update rates"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"model-deployment-edge-vs-cloud",children:"Model Deployment (Edge vs Cloud)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Edge Deployment"}),": Models run on robot hardware"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Advantages: Low latency, offline operation, privacy"}),"\n",(0,r.jsx)(n.li,{children:"Disadvantages: Limited compute, power consumption, model size constraints"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cloud Deployment"}),": Models run on remote servers"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Advantages: Powerful compute, easy updates, large models"}),"\n",(0,r.jsx)(n.li,{children:"Disadvantages: Network latency, connectivity requirements, privacy concerns"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"failure-modes-and-fallbacks",children:"Failure Modes and Fallbacks"}),"\n",(0,r.jsx)(n.p,{children:"Common AI failure modes in robotics:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perception Failures"}),": Misidentifying objects or misestimating states"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Planning Failures"}),": Generating infeasible or unsafe plans"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Control Failures"}),": Inability to execute planned actions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Communication Failures"}),": Loss of connectivity for cloud-based models"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"code-snippets",children:"Code Snippets"}),"\n",(0,r.jsx)(n.p,{children:"AI inference loop implementation:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport torch\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass AIPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'ai_perception_node\')\n\n        # Initialize AI model\n        self.model = self.load_model()\n        self.model.eval()\n\n        # Initialize ROS 2 interfaces\n        self.bridge = CvBridge()\n        self.image_sub = self.create_subscription(\n            Image, \'camera/image_raw\', self.image_callback, 10)\n        self.command_pub = self.create_publisher(String, \'ai_commands\', 10)\n        self.velocity_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n\n        # Performance monitoring\n        self.inference_times = []\n\n        # Timer for periodic processing\n        self.timer = self.create_timer(0.1, self.process_pending_data)  # 10Hz\n        self.pending_image = None\n\n    def load_model(self):\n        """Load pre-trained AI model for perception tasks"""\n        # Load a pre-trained model (example: object detection)\n        # In practice, this would load your specific model\n        model = torch.hub.load(\'ultralytics/yolov5\', \'yolov5s\', pretrained=True)\n        return model\n\n    def image_callback(self, msg):\n        """Receive image from camera and store for processing"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n            self.pending_image = cv_image\n        except Exception as e:\n            self.get_logger().error(f"Error converting image: {e}")\n\n    def process_pending_data(self):\n        """Process pending image data with AI model"""\n        if self.pending_image is not None:\n            start_time = self.get_clock().now()\n\n            try:\n                # Preprocess image for model\n                input_tensor = self.preprocess_image(self.pending_image)\n\n                # Run AI inference\n                with torch.no_grad():\n                    results = self.model(input_tensor)\n\n                # Process results\n                processed_results = self.process_model_output(results)\n\n                # Publish AI commands based on results\n                self.publish_commands(processed_results)\n\n                # Calculate and log inference time\n                end_time = self.get_clock().now()\n                inference_time = (end_time.nanoseconds - start_time.nanoseconds) / 1e9\n                self.inference_times.append(inference_time)\n\n                # Log performance metrics periodically\n                if len(self.inference_times) % 10 == 0:\n                    avg_time = sum(self.inference_times[-10:]) / 10\n                    self.get_logger().info(f"Average inference time: {avg_time:.3f}s")\n\n            except Exception as e:\n                self.get_logger().error(f"Error in AI processing: {e}")\n                # Fallback behavior\n                self.fallback_behavior()\n\n            finally:\n                self.pending_image = None\n\n    def preprocess_image(self, image):\n        """Preprocess image for AI model input"""\n        # Resize image to model input size\n        resized = cv2.resize(image, (640, 640))\n\n        # Convert to tensor and normalize\n        tensor = torch.from_numpy(resized).permute(2, 0, 1).float() / 255.0\n        tensor = tensor.unsqueeze(0)  # Add batch dimension\n\n        return tensor\n\n    def process_model_output(self, results):\n        """Process AI model output and extract meaningful information"""\n        # Convert results to list of detections\n        detections = results.pandas().xyxy[0].to_dict(\'records\')\n\n        # Filter detections based on confidence\n        high_conf_detections = [\n            det for det in detections\n            if det[\'confidence\'] > 0.5\n        ]\n\n        return high_conf_detections\n\n    def publish_commands(self, detections):\n        """Publish AI-generated commands based on detections"""\n        if not detections:\n            # No objects detected, publish neutral command\n            cmd_msg = String()\n            cmd_msg.data = "no_objects_detected"\n            self.command_pub.publish(cmd_msg)\n            return\n\n        # Find the closest object\n        closest_obj = min(detections, key=lambda x: x[\'ymin\'])\n\n        # Generate command based on object type and position\n        cmd_msg = String()\n        cmd_msg.data = f"approach_{closest_obj[\'name\']}_at_{closest_obj[\'xmin\']}_{closest_obj[\'ymin\']}"\n        self.command_pub.publish(cmd_msg)\n\n        # Also publish velocity command if it\'s a navigation task\n        vel_msg = Twist()\n        # Calculate velocity based on object position\n        center_x = closest_obj[\'xmin\'] + (closest_obj[\'xmax\'] - closest_obj[\'xmin\']) / 2\n        if center_x < 213:  # Left third of image\n            vel_msg.angular.z = 0.5  # Turn left\n        elif center_x > 426:  # Right third of image\n            vel_msg.angular.z = -0.5  # Turn right\n        else:\n            vel_msg.linear.x = 0.2  # Move forward\n\n        self.velocity_pub.publish(vel_msg)\n\n    def fallback_behavior(self):\n        """Implement safe fallback behavior when AI fails"""\n        # Stop robot movement\n        vel_msg = Twist()\n        vel_msg.linear.x = 0.0\n        vel_msg.angular.z = 0.0\n        self.velocity_pub.publish(vel_msg)\n\n        # Log the fallback event\n        self.get_logger().warn("AI system failed, activating fallback behavior")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    ai_node = AIPerceptionNode()\n\n    try:\n        rclpy.spin(ai_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        ai_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h4,{id:"model-monitoring-and-safety",children:"Model Monitoring and Safety"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance Monitoring"}),": Track inference times, accuracy, and resource usage"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Anomaly Detection"}),": Identify when AI models behave unexpectedly"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety Checks"}),": Validate AI outputs before execution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fallback Systems"}),": Ensure safe behavior when AI fails"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"senior--executive",children:"Senior / Executive"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Audience"}),": System architect / decision maker"]}),"\n",(0,r.jsx)(n.h4,{id:"learning-objectives-3",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Design scalable AI system architectures for humanoid robotics"}),"\n",(0,r.jsx)(n.li,{children:"Implement safety, verification, and monitoring frameworks"}),"\n",(0,r.jsx)(n.li,{children:"Plan for cost, scalability, and model update strategies"}),"\n",(0,r.jsx)(n.li,{children:"Establish evaluation metrics for AI-robotics systems"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate build vs buy decisions for AI components"}),"\n",(0,r.jsx)(n.li,{children:"Balance performance requirements with safety constraints"}),"\n",(0,r.jsx)(n.li,{children:"Plan for long-term AI system maintenance and evolution"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"ai-system-architecture-for-humanoids",children:"AI System Architecture for Humanoids"}),"\n",(0,r.jsx)(n.p,{children:"A comprehensive AI architecture includes:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perception Layer"}),": Vision, audio, and sensor processing systems"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Understanding Layer"}),": Natural language, scene understanding, and state estimation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Planning Layer"}),": Task, motion, and behavior planning systems"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Learning Layer"}),": Online and offline learning capabilities"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Execution Layer"}),": Control systems and safety monitors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integration Layer"}),": ROS 2 interfaces and communication systems"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"safety-verification-and-monitoring",children:"Safety, Verification, and Monitoring"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Safety Framework"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Risk Assessment"}),": Identify potential failure modes and their consequences"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety Requirements"}),": Define safety constraints for AI behavior"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Verification Methods"}),": Testing and validation procedures for AI systems"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Monitoring Systems"}),": Real-time safety and performance monitoring"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Verification Approaches"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simulation Testing"}),": Extensive testing in simulated environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware-in-the-Loop"}),": Testing with real hardware components"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Formal Methods"}),": Mathematical verification of safety properties"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Adversarial Testing"}),": Testing with challenging or unexpected inputs"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"cost-scalability-and-update-strategy",children:"Cost, Scalability, and Update Strategy"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cost Considerations"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Development Costs"}),": AI model development, training, and validation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computational Costs"}),": Hardware requirements and power consumption"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Maintenance Costs"}),": Ongoing model updates and system maintenance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Personnel Costs"}),": AI/robotics expertise requirements"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Scalability Planning"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Scaling"}),": Ability to handle increasing complexity and data volume"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deployment Scaling"}),": Supporting multiple robots with shared AI systems"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Update Scaling"}),": Efficient distribution of model updates across robot fleet"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"evaluation-metrics-task-success-robustness",children:"Evaluation Metrics (Task Success, Robustness)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance Metrics"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Task Success Rate"}),": Percentage of tasks completed successfully"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Efficiency"}),": Time and resources required for task completion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy"}),": Correctness of perception and decision-making"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Adaptability"}),": Ability to handle novel situations"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Robustness Metrics"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Failure Rate"}),": Frequency of system failures or incorrect behaviors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Recovery Time"}),": Time to recover from failures or unexpected situations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Generalization"}),": Performance on unseen environments or tasks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stress Testing"}),": Performance under challenging conditions"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"build-vs-buy-considerations",children:"Build vs Buy Considerations"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Build Advantages"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Customization to specific requirements"}),"\n",(0,r.jsx)(n.li,{children:"Full control over model behavior"}),"\n",(0,r.jsx)(n.li,{children:"Proprietary competitive advantage"}),"\n",(0,r.jsx)(n.li,{children:"Long-term maintainability"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Buy Advantages"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Faster time to market"}),"\n",(0,r.jsx)(n.li,{children:"Established technology and support"}),"\n",(0,r.jsx)(n.li,{children:"Lower development risk"}),"\n",(0,r.jsx)(n.li,{children:"Cost-effective for commodity functionality"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Hybrid Approach"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use commercial solutions for commodity AI functions"}),"\n",(0,r.jsx)(n.li,{children:"Develop custom solutions for differentiating capabilities"}),"\n",(0,r.jsx)(n.li,{children:"Maintain flexibility to switch between approaches"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"deployment-readiness-checklist",children:"Deployment Readiness Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:"Safety Validation"}),": Has the AI system been validated for safe operation?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:"Performance Requirements"}),": Does the system meet latency and accuracy requirements?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:"Hardware Constraints"}),": Is the system compatible with robot hardware limitations?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:"Robustness Testing"}),": Has the system been tested under various conditions?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:"Update Mechanism"}),": Is there a system for updating AI models in deployment?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:"Monitoring Tools"}),": Are there appropriate tools for monitoring AI performance?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:"Fallback Systems"}),": Are there safe fallback behaviors when AI fails?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:"Regulatory Compliance"}),": Does the system meet relevant safety standards?"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);